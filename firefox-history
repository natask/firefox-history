#! /usr/bin/env python3
# coding= utf-8
'''
usage: firefox-history [-h] [--database Database] [--postfix Postfix] [--depth DEPTH] [--stable] [--elisp] [--visit] [--chrono] [--backtrace] [--query] [--title] [--timeout TIMEOUT] [URL] [--time TIME]

Firefox History v0.2

Copyright (C) 2021  Natnael Kahssay

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.

positional arguments:
  URL                  Url that is to be searched in database.
                        [default: https://gist.github.com/olejorgenb/9418bef65c65cd1f489557cfc08dde96]

optional arguments:
  -h, --help           show this help message and exit
  --database Database  Location of database.
                        [default: /home/savnkk/.mozilla/firefox/ynulz4el.dev-edition-default/places.sqlite]
  --postfix Postfix    Postfix difference between main database and copy database to which sql will connect to.
                        [default: bak]
  --depth DEPTH        depth of backtrace and chronology for `url. [default: 10]
  --time TIME          search TIME instead of URL. [default: 0]
  --stable             Database is stable. It doesn't change therefore direct connection will be made to it. [default: False]
  --elisp              Output in elisp form. [default: False]
  --visit              Output visited dates for `url. [default: False]
  --chrono             Output chronology for `url. [default: False]
  --backtrace          Output backtrace for `url. [default: False]
  --query              Output query result for `query by intrepreting `url as a `query. [default: False]
  --title              Include url title in output. Costly because need to make url query. [default: False]
  --timeout TIMEOUT    timeout for url title query. [default: 0.5]

example:
 firefox-history https://google.com
'''
import sqlite3
import subprocess
import shlex
from datetime import datetime
from docopt import docopt

## title from url
from multiprocessing import Pool

def process_data_subprocess(url):
 try:
  res = subprocess.getoutput("xidel " + "\"" + url + "\"" + "  -s --extract //title --error-handling=500| head -1")
  if "Error:\nInternet/HTTP Error" in res:
    res = url;
  return res;
 except:
  return url;

def get_titles_subprocess(urls):
 pool_len = len(urls)
 with Pool(pool_len) as p:
  return p.map(process_data_subprocess, urls)

# import urllib.request as urllib
# from lxml.html import document_fromstring
# def getUrlTitle(url):
#  page = urllib.urlopen(url, timeout=TIMEOUT).read().decode('utf-8')
#  p = document_fromstring(page)
#  return p.find(".//title").text

# def process_data(url):
#  try:
#   return getUrlTitle(url)
#  except:
#   return url;
# def get_titles_python(urls):
#  pool_len = len(urls)
#  with Pool(pool_len) as p:
#   return p.map(process_data, urls)

# import urlTitle
# def get_titles_cpp(urls):
#  return urlTitle.urlTitle_getUrlTitles(urls);

def dateToString(date):
 return(datetime.utcfromtimestamp(date/1000000).strftime('%Y-%m-%d %H:%M:%S'));

def getQueryResults(cur, string):
 cur.execute(string);
 ret =  cur.fetchall();
 return ret;

def getDateUrlTitle(cur, string):
 ret = getQueryResults(cur, "select visit_date, url from mine " + string);
 urls = {};
 if TITLEP and ret:
  urls = {item[1] for item in ret}
 return (ret, urls);

'''
Get visit date and url for urls that are like URL.
'''
def getVisitInfo(cur, url):
 global URLS
 ret, urls =  getDateUrlTitle(cur, f"where url like {url}");
 URLS.update(urls);
 return ret;

'''
Get visit date and url for date that is TIME.
'''
def getVisitInfoTime(cur, time):
 global URLS
 ret, urls=  getDateUrlTitle(cur, f"where visit_date = {time}");
 URLS.update(urls);
 return ret;


'''
Get chronological info for VISIT_INFO.
'''
def getChrono(cur, visit_info):
 global URLS
 chrono = {};
 for item in visit_info:
  date = item[0]
  chrono[date] = {"item": item, "left" : [ ], "right" : []}
  chrono_date = chrono[date]
  chrono_date["right"],urls_right = getDateUrlTitle(cur, f"where visit_date > {date} limit {DEPTH}")
  chrono_date["left"], urls_left = getDateUrlTitle(cur, f"where visit_date < {date} order by visit_date desc limit {DEPTH}")
  URLS.update(urls_left)
  URLS.update(urls_right)
  chrono_date["left"] = chrono_date["left"][::-1]
 return chrono;

'''
Get backtrace info for VISIT_INFO.
'''
def getBacktrace(cur, visit_info):
 global URLS
 backtrace = {}
 for item in visit_info:
  date = item[0]
  res = cur.execute(f"select hid from mine where visit_date = {date} limit 10")
  if res := cur.fetchone():
   hid = res[0]

   backtrace[date] = {"item": item, "backtrace": []}
   backtrace_date = backtrace[date]["backtrace"]
   depth = 0;
   while hid != 0 or depth < DEPTH:
    depth = depth + 1;
    cur.execute(f"select from_visit from mine where hid={hid}")
    if res := cur.fetchone():
     hid = res[0]
    else:
     hid = 0;

    if (hid != 0):
     res, urls = getDateUrlTitle(cur, f"where hid={hid}")
     backtrace_date += (res)
     URLS.update(urls)


 return backtrace;

def print_chrono_normal(chrono):
 print()
 print("Chronological track")
 print("##############################")

 for date in chrono.keys():
  for item in chrono[date]["left"]:
   print(dateToString(item[0]), item)

  print()
  print(dateToString(date), chrono[date]["item"])
  print()
  for item in chrono[date]["right"]:
   print(dateToString(item[0]), item)

  print()
  print()
  print()
  print("##############################")

def print_backtrace_normal(backtrace):
 print()
 print()
 print("Back track")
 print("##############################")
 for date in backtrace.keys():
  print((dateToString(date), backtrace[date]["url"]))
  print()

  for entry in backtrace[date]["backtrace"]:
   print((dateToString(entry[0]), entry[1]))

  print()
  print()
  print()

 print("##############################")

def is_iterable(item):
 return hasattr(item, '__getitem__');

def print_iter_elisp_helper(item):
 if not is_iterable(item):
  print(item, end=" ")
 elif type(item) == type(""):
  print("\"" + item.strip() + "\"", end=" ")
 elif type(item) == type({}):
  print("(", end="");
  for val in item:
   print("(", end="");
   print_iter_elisp_helper(val);
   print_iter_elisp_helper(item[val]);
   print(")", end=" ");

  print(")", end="")
 else: #type(item) == type([])
  print("(", end="");
  for val in item:
   print_iter_elisp_helper(val);

  print(")", end= "")


def print_iter_elisp(item):
 print_iter_elisp_helper(item);
 print();

def main():
 global DEPTH, TITLEP, TIMEOUT, URLS
 args       = docopt(__doc__, version="Firefox History v0.2")
 query      = args["URL"] or "https://gist.github.com/olejorgenb/9418bef65c65cd1f489557cfc08dde96"
 url        = "'" + query + "'"
 database   = args["--database"]
 postfix    = args["--postfix"]
 DEPTH      = int(args["--depth"])
 stable     = args["--stable"]
 elisp      = args["--elisp"]
 backtracep = args["--backtrace"]
 queryp     = args["--query"]
 visitp     = args["--visit"]
 chronop    = args["--chrono"]
 time       = int(args["--time"])
 TITLEP     = args["--title"]
 TIMEOUT    = float(args["--timeout"])

 URLS = set()
 if not stable:
  # NOTE: need to check there is no other process making calls to the copy database.
  subprocess.call(shlex.split(f"cp {database} {database + '.' + postfix}"))

 database = database + '.' + postfix
 con = sqlite3.connect(database)
 cur = con.cursor()
 try:
  cur.execute(f"create view mine as select a.id, b.id hid, a.url, b.from_visit, b.visit_date from moz_places a inner join moz_historyvisits b on b.place_id = a.id order by b.visit_date")
 except:
  pass

 if time==0:
  visit_info = getVisitInfo(cur,url);
 else:
  visit_info = getVisitInfoTime(cur,time);

 chrono = chronop and getChrono(cur, visit_info);
 backtrace = backtracep and getBacktrace(cur,visit_info);
 queryResults = queryp and getQueryResults(cur,query);
 url_title = TITLEP and [alternate for url_title in zip(URLS, get_titles_subprocess(URLS)) for alternate in url_title]


 if not elisp:
  queryp and print(queryResults);
  visitp and  print(visit_info);
  chronop and print_chrono_normal(chrono);
  backtracep and print_backtrace_normal(backtrace);
  TITLEP and  print(url_title);
 else:
  queryp and print_iter_elisp(queryResults);
  visitp and print_iter_elisp(visit_info);
  chronop and print_iter_elisp(chrono);
  backtracep and print_iter_elisp(backtrace);
  TITLEP and print_iter_elisp(url_title);

 cur.close()


if __name__ == "__main__":
 main();
